# 实验记录
PS:实验中保持随机种子一致，以消除随机性导致的差距
## 分布一
```
m1 = np.array([-5, 0])
m2 = np.array([0, 5])
cov1 = np.eye(2)
cov2 = np.eye(2)
one_sample_num = 200
```
### 超参设置：lr=0.1
1. epoch_num=100, batch_size=25: acc=1.0
2. epoch_num=10, batch_size=25:  acc=1.0
3. epoch_num=10, batch_size=10: acc=1.0(loss收敛比配置2更快)

## 分布二
```
m1 = np.array([-1, 0])
m2 = np.array([0, 2])
cov1 = np.eye(2)
cov2 = np.eye(2)
one_sample_num = 200
```
### 超参设置：lr=0.1
1. epoch_num=100, batch_size=25: acc=0.85
2. epoch_num=100, batch_size=10: acc=0.85
3. epoch_num=10, batch_size=25:  acc=0.8375
4. epoch_num=10, batch_size=10: acc=0.8375
5. epoch_num=2, batch_size=25: acc=0.775
6. epoch_num=2, batch_size=10: acc=0.825

# 实验分析
1. 由于分布一正负样本分布差距过大，分类任务过于简单，因此不管用怎样的超参数，分类器都能完美分类；但是通过观察Loss下降曲线，可以得知batch_size较小的模型收敛得更快。
2. 分布二分类难度适中，可以区分不同超参数下的模型性能。
* 在学习率、batch_size一定时，模型准确度与训练轮次epoch_num呈正相关，但当epoch_num足够大时，模型会收敛至上限。
* 在学习率、epoch_num一定时，更小的batch_size会带来更好的模型准确性与收敛速度，这与SGD选用更小的batch_size可以引入适当的噪声与更多的训练轮次有着密切联系，但同时这会带来更高的计算消耗。